# ğŸ§  My First AI Brain

This is my first neural network â€” fully coded **from scratch** in Python using only NumPy.  
No frameworks, no magic. I built this to really understand how an AI learns.

## âš™ï¸ What it does
A simple feed-forward network that learns to solve the **XOR logic problem**.  
It starts out guessing randomly, then teaches itself the correct pattern through  
**forward propagation**, **backpropagation**, and **gradient descent**.

Input â†’ Hidden (4 neurons) â†’ Output (1 neuron)

After training, it predicts:

[0, 1, 1, 0] âœ… (100% accuracy)

## ğŸ§© How it works
1. **Initialization** â€” Xavier weights + tiny random biases for stability.  
2. **Activation** â€” Sigmoid neurons turn raw numbers into smooth probabilities.  
3. **Loss** â€” Binary Cross-Entropy measures how wrong the model is.  
4. **Backpropagation** â€” calculates how each weight affected the error.  
5. **Gradient descent** â€” updates the weights to make fewer mistakes each round.

---

## ğŸ”§ Run it yourself
pip install numpy
python network.py


Output shows the training progress and final accuracy.

ğŸ“ What I learned

How neurons, activations, weights, and biases actually work

How learning = minimizing error step by step

How to build AI logic without using TensorFlow or PyTorch

ğŸ—ï¸ Next step

Next up: scaling this idea to recognize handwritten digits (multi-class problem)
in the file digits_softmax.py.








Â© 2025 Kevin Mast â€“ Built as part of my AI learning journey.
